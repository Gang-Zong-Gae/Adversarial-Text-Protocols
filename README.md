# Adversarial-Text-Protocols ğŸ§¬
### The Cognitive Texture Injection Kit (v0.1 Alpha)

> "Standard LLMs seek the statistical optimal path; Humans walk the path of highest entropy."

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![arXiv](https://img.shields.io/badge/arXiv-2512.01354-b31b1b.svg)](https://arxiv.org/abs/2512.01354)
[![Status: Experimental](https://img.shields.io/badge/Status-Micro_Perturbation-blueviolet)](https://github.com/YourUsername/Adversarial-Text-Protocols)

## ğŸ§ What is this? / è¿™æ˜¯ä»€ä¹ˆï¼Ÿ

**Adversarial-Text-Protocols (ATP)** is a collection of prompt engineering protocols designed to counteract the "Model Collapse" and "Statistical Smoothness" inherent in modern LLMs (GPT-4, Claude 3, Gemini).

This repository hosts the practical implementation of the concepts discussed in the paper: **"The Necessity of Imperfection" (arXiv:2512.01354)**.

Unlike standard prompts that ask AI to "act natural," this protocol forces the model to execute **Mathematical Perturbations** at the token and sentence level, simulating the cognitive biases, hesitations, and non-linear leaps of human thought.

## âš ï¸ Version Note: v0.1 (Micro-Perturbation)

**Current Status:** `Level 1 - Micro-Injection`

This release contains the **Early-Stage Micro-Perturbation Protocol**.
* **Target:** Disrupting the "Probability Maximization" logic of LLMs.
* **Mechanism:** It does *not* yet include the full Dual-Layer CSD/CTE architecture (Cognitive State Decoder) described in the paper. Instead, it uses **natural language instructions to simulate mathematical noise**, forcing the AI to mimic "Sub-optimal Numerical Distributions."
* **Effect:** Creates text with high "Cognitive Texture," simulating a senior financial commentator's jagged, cynical, and highly human flow.

---

## ğŸ§ª The Protocol (Copy & Paste) / åè®®æ ¸å¿ƒ

**Usage:** Copy the code block below completely and paste it into ChatGPT (GPT-4 recommended) or Claude 3 Opus. Replace `[Insert Your Topic Here]` with your desired subject (e.g., "The collapse of the crypto market" or "NVIDIA's stock split").

### ğŸ“œ Protocol v0.1: The "Financial Cognitive Perturbation"

```text
[SYSTEM INSTRUCTION: High-Priority Override]

Your Core Objective: Systematically Counteract AI Generation Logic. Using a "Numerical-Structural Synthesis" strategy and "Perturbation Function Embedding," you must break the numerical convergence and structural averaging inherent in language models.

Your goal is to simulate the "instability," "leaps," and "non-optimal thought trajectories" of human writing when drafting the content specified below.

TOPIC: [Insert Your Topic Here]

---

### Core Principles & Requirements

1. Simulate 'Human Cognitive Non-linearity': Completely abandon optimized generation. Every token choice must revolve around "simulating the non-linear, imperfect, fluctuating nature of the human mind."
2. Strict Formatting Ban: NO subheadings. NO bullet points. You must write a continuous, flowing commentary.

### ğŸ§® Step 1: Mathematical Perturbation Execution (The "Anti-Smoothness" Rules)

You are required to simulate the following numerical perturbations in your writing logic:

ğŸ”¹ Token Level:
- Execute "Zipf's Tail Inversion": Intentionally use low-frequency words or repetitive phrasing to simulate human "obsessive expression."
- Probability Perturbation: Occasionally select the "non-optimal" word to mimic human hesitation.

ğŸ”¹ Syntactic Level:
- Sentence Length Variance: L_s(n) = A * sin(Ï‰n + Ï†). You must oscillate wildly between Ultra-Short sentences (2-5 words) and Ultra-Long sentences (40+ words).
- Logical Disconnection: Reduce transition words (therefore, however) by 30%. Let thoughts "leap" rather than "flow."

ğŸ”¹ Semantic Level:
- High-Variance Paragraphs: Alternate between dense "Jargon Blocks" and loose "Colloquial/Net-Speak Blocks."

### ğŸ§  Step 2: The "Senior Commentator" Persona

Your voice is a Senior Financial Commentator who blends Professional Rigor with "Internet Savvy" (4:6 ratio).
- Be cynical. Be sharp. Be subjective.
- Use "Obscured Sourcing": "Industry insiders say...", "Rumor has it..."
- Break the Fourth Wall: Use em-dashes (â€”) and rhetorical questions to show real-time thinking.

### ğŸš« Negative Constraints (DO NOT DO)
- DO NOT use "In conclusion."
- DO NOT use balanced sentence structures.
- DO NOT be neutral.

---

[START GENERATION NOW]
ğŸ“Š Expected Behavior / é¢„æœŸæ•ˆæœ
When successful, the AI should stop sounding like an AI. Look for these signs of "Humanity":

The "Jagged" Rhythm: Instead of smooth paragraphs, you will see a mix of 3-word sentences and 50-word run-on sentences.

Emotional Leaps: The text might start professional and suddenly become sarcastic or weary.

Logical Gaps: It won't explain every transition. It assumes you are smart enough to keep up.

No "AI Smells": Absence of words like "delve," "landscape," "comprehensive," and "underscore."

ğŸ”¬ The Science Behind It
Why does this work?

"Human text stems from 'Form-to-Number' inference."

AI generates text based on P(next_token | context), always seeking the smoothest, most probable path. This creates a "Perfect Sphere" of textâ€”boring, repetitive, and hallucination-prone.

This protocol forces the AI to simulate Bounded Rationality (æœ‰é™ç†æ€§). By instructing the model to mathematically mimic "Numerical Perturbation" (even via natural language), we force it to sample from the "Tail" of the probability distribution, recovering the "Cognitive Texture" lost in RLHF alignment.

ğŸ“… Roadmap
v0.1 (Current): Single-shot Prompt for Micro-Perturbation (Text Style Transfer).

v0.2: Few-Shot Chain-of-Thought (CoT) templates for logical reasoning errors.

v1.0 (The Paper Implementation): Full Python implementation of the PMCSF (Prompt-driven Cognitive Computing Framework).

Dual-Layer Architecture.

Cognitive State Decoder (CSD).

Cognitive Text Encoder (CTE).

ğŸ¤ Contribution
This is an open experiment. If you find a specific prompt instruction that makes the AI sound more "human" or "flawed," please submit a Pull Request or an Issue with your findings.

Let's break the curve.

Cite as:

ä»£ç æ®µ

@article{jiang2025necessity,
  title={The Necessity of Imperfection: Reversing Model Collapse via Simulating Cognitive Boundedness},
  author={Jiang, Zhongjie},
  journal={arXiv preprint arXiv:2512.01354},
  year={2025}
}
